{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd6e90a",
   "metadata": {},
   "source": [
    "# LCPB 23-24 exercise 2 (Data visualization and clustering)\n",
    "- Andrea Semenzato 2130973\n",
    "- Pietro Bernardi 2097494\n",
    "- Tomàs Mezquita 2109239\n",
    "- Mariam Chokheli 2122278"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103e8b1-9f08-4636-b070-10729c95d820",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea0956b-a332-41ed-80ec-403159fdde62",
   "metadata": {},
   "source": [
    "### POINT 1\n",
    "\n",
    "### “Eps” ($\\epsilon$) and “minPts” (mP) in DBSCAN algorithm for clustering\n",
    "\n",
    "### Refine the grid with more values of ε and mP and plot a heat-map showing the normalized mutual information (NMI) between true and predicted clusters, similar to the one on the right.\n",
    "\n",
    "### Is the high NMI region showing a correlation between ε and mP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202422db-3daf-49d1-a51b-dd44ed8ed9f7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The correlation between the two parameters is clear, displaying a linear relationship. The visualization highlights a zone with nonzero NMIs, indicating a linear trend. This suggests that increasing epsilon necessitates a corresponding increase in the minimum cluster points for effective results. Notably, optimal outcomes are achieved with a lower count of minimum points.\n",
    "\n",
    "\n",
    "<img src=\"images/img1ex2.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e0dea3-9ae5-4578-a62f-ac34b6cfc8c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Code to generate the heat map\n",
    "\n",
    "eps_range=np.linspace(r/2,2.5*r,30)\n",
    "min_sample_range = np.arange(1,50,2, dtype=int)\n",
    "\n",
    "XX,YY=np.meshgrid(min_sample_range, eps_range)\n",
    "\n",
    "DIMY=len(eps_range)\n",
    "DIMX=len(min_sample_range)\n",
    "it=0\n",
    "\n",
    "nmi=np.zeros((DIMY,DIMX))\n",
    "for i, eps in enumerate(eps_range):\n",
    "    for j, min_samples in enumerate(min_sample_range):\n",
    "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        model.fit(X)\n",
    "        y_hat = model.labels_\n",
    "        nmi[i,j]=NMI(y_hat, y_true)\n",
    "        \n",
    "\n",
    "plt.pcolormesh(XX,YY,nmi)\n",
    "plt.xlabel('Min samples')\n",
    "plt.ylabel('Eps')\n",
    "plt.title('NMI Heat map')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc314e1-a7e4-459d-a4c3-7eea4f28fc61",
   "metadata": {},
   "source": [
    "### POINT 2\n",
    "\n",
    "### Understanding the 12-dimensional data Use the principal component analysis (PCA) to visualize the first components of the data.\n",
    "### Does it help understand its structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f47398-b586-4fab-992e-fc4b632277b3",
   "metadata": {},
   "source": [
    "The comparison between 2D and 3D PCA highlights the importance of dimensionality selection in PCA application. Despite 2D PCA capturing about 80% of the dataset's variance, it does not fully reveal the data's complexity. In contrast, 3D PCA, accounting for approximately 95% of the variance, offers a deeper, more detailed understanding of the dataset's patterns and relationships.\n",
    "\n",
    "<img src=\"images/2d3d.png\" width=\"1200\">\n",
    "\n",
    "This emphasizes how 3D PCA can uncover insights that may be missed with lower-dimensional analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7ca995-b781-48fc-8ad7-42ffdb5dbe60",
   "metadata": {},
   "source": [
    "# Code for 2D and 3D PCA on the 12-dimensional dataset below\n",
    "\n",
    "data = np.loadtxt(\"x_12d.dat\", delimiter='\\t')\n",
    "y = np.loadtxt(\"y_12d.dat\", dtype=int)\n",
    "colors = ['r', 'b', 'gold']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data)\n",
    "\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_2d = pca_2d.fit_transform(X)\n",
    "X_pca_3d = pca_3d.fit_transform(X)\n",
    "\n",
    "# 2D PCA \n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "unique_labels = np.unique(y)\n",
    "\n",
    "ax1 = fig.add_subplot(2, 4, 1)\n",
    "for i, color in zip(unique_labels, colors):\n",
    "    ax1.scatter(X_pca_2d[y == i, 0], X_pca_2d[y == i, 1], c=color, alpha=0.8, label=f'Class {i}')\n",
    "ax1.set_title('Combined 2D PCA Visualization')\n",
    "ax1.set_xlabel('Principal Component 1')\n",
    "ax1.set_ylabel('Principal Component 2')\n",
    "ax1.grid(True)\n",
    "\n",
    "for index, label in enumerate(unique_labels):\n",
    "    ax = fig.add_subplot(2, 4, index + 2)\n",
    "    ax.scatter(X_pca_2d[y == label, 0], X_pca_2d[y == label, 1], c=colors[index], alpha=0.8, label=f'Class {label}')\n",
    "    ax.set_title(f'2D PCA Class {label}')\n",
    "    ax.set_xlabel('Principal Component 1')\n",
    "    ax.set_ylabel('Principal Component 2')\n",
    "    ax.grid(True)\n",
    "\n",
    "# 3D PCA\n",
    "fig_3d = plt.figure(figsize=(16, 8))\n",
    "\n",
    "ax2 = fig_3d.add_subplot(2, 4, 1, projection='3d')\n",
    "for i, color in zip(unique_labels, colors):\n",
    "    ax2.scatter(X_pca_3d[y == i, 0], X_pca_3d[y == i, 1], X_pca_3d[y == i, 2], c=color, alpha=0.8, label=f'Class {i}')\n",
    "ax2.set_title('Combined 3D PCA Visualization')\n",
    "ax2.set_xlabel('Principal Component 1')\n",
    "ax2.set_ylabel('Principal Component 2')\n",
    "ax2.set_zlabel('Principal Component 3')\n",
    "ax2.grid(True)\n",
    "\n",
    "for index, label in enumerate(unique_labels):\n",
    "    ax = fig_3d.add_subplot(2, 4, index + 2, projection='3d')\n",
    "    ax.scatter(X_pca_3d[y == label, 0], X_pca_3d[y == label, 1], X_pca_3d[y == label, 2], c=colors[index], alpha=0.8, label=f'Class {label}')\n",
    "    ax.set_title(f'3D PCA Class {label}')\n",
    "    ax.set_xlabel('Principal Component 1')\n",
    "    ax.set_ylabel('Principal Component 2')\n",
    "    ax.set_zlabel('Principal Component 3')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Variances\n",
    "print(\"Explained variance ratio for 2D PCA:\", pca_2d.explained_variance_ratio_)\n",
    "print(\"Explained variance ratio for 3D PCA:\", pca_3d.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad661dba-41c3-4912-a5f6-b6f0908f270a",
   "metadata": {},
   "source": [
    "### POINT 1\n",
    "\n",
    "### Compare different clustering methods\n",
    "### a) Perform a k-means clustering of the data, with k=3. Does it work better than DBSCAN? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020932f-53f9-4bab-8f15-d34335577077",
   "metadata": {},
   "source": [
    "<img src=\"images/dbscan.png\" width=\"400\" style=\"display:inline-block\">\n",
    "<img src=\"images/kmeans.png\" width=\"500\" style=\"display:inline-block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9decc43-2015-4ca4-85d9-2de091dcca5d",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(data)\n",
    "labels = kmeans.predict(data)\n",
    "\n",
    "markers = ['o', 's', '^']  # For example: circle, square, triangle\n",
    "colors = ['blue', 'red', 'gold']\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(k):\n",
    "    plt.scatter(data[labels == i, 0], data[labels == i, 1], s=50, c=colors[i], marker=markers[i], edgecolor='k', label=f'Cluster {i+1}')\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c=\"green\", s=200, marker='X', edgecolor='k', label='Centroids')\n",
    "plt.title(\"K-means Clustering (k=3)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00a40f-f807-490e-8218-550b07177217",
   "metadata": {},
   "source": [
    "### b) Perform a hierarchical clustering of the data and plot the corresponding dendrogram. Does it work better than DBSCAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1c336-b7b6-4ed0-b47f-865d0daeec9f",
   "metadata": {},
   "source": [
    "<img src=\"images/dendogram.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035920e8-72db-40fc-bb47-316724f4ffd3",
   "metadata": {},
   "source": [
    "Z = linkage(data, 'ward')\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=12,\n",
    "    leaf_rotation=90., \n",
    "    leaf_font_size=12.,\n",
    ")\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Cluster size')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
