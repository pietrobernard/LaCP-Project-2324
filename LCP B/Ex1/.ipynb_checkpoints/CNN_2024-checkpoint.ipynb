{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 13\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten #, Reshape\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Show_data(x,L,s=\"data\",T=3):\n",
    "    c=(\"k\",\"r\",\"b\")\n",
    "    for i in range(T):\n",
    "        j=i*3*L\n",
    "        plt.plot(np.arange(j,j+L),x[i*3],c=c[0])\n",
    "        plt.plot(np.arange(j+L,j+2*L),x[i*3+1],c=c[1])\n",
    "        plt.plot(np.arange(j+2*L,j+3*L),x[i*3+2],c=c[2])\n",
    "    plt.title(s)\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.show()\n",
    "    \n",
    "str0=\"ts_L80_N4000.dat\"\n",
    "fnamex='x_'+str0\n",
    "fnamey='y_'+str0\n",
    "\n",
    "x = np.loadtxt(fnamex, delimiter=\" \",dtype=float)\n",
    "N,L = len(x), len(x[0])\n",
    "\n",
    "Show_data(x,L,\"part of the original data\")\n",
    "\n",
    "categ = np.loadtxt(fnamey, dtype=int)\n",
    "n_class = 3    # y.argmax() - y.argmin() +1\n",
    "print('data: ',N)\n",
    "\n",
    "y = np.zeros((N,n_class))\n",
    "for i in range(N):\n",
    "    y[i][categ[i]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale data, split train/val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  FIRST PASSAGE:  DO NOT DO THIS --> FAILURE \n",
    "#\n",
    "# bring each sample to zero average\n",
    "xm = x.mean(axis=1)\n",
    "for i in range(N):\n",
    "    x[i] = x[i]-xm[i]\n",
    "\n",
    "#\n",
    "#  SECOND PASSAGE:  DO NOT DO THIS --> ALSO FAILURE \n",
    "#\n",
    "#rescale (crude version, variance should be used)\n",
    "x = x / x.std()\n",
    "    \n",
    "Show_data(x,L,\"rescaled data\")\n",
    "\n",
    "perc_train=0.8\n",
    "N_train = int(perc_train*N)\n",
    "x_train = x[:N_train]\n",
    "y_train = y[:N_train]\n",
    "x_val = x[N_train:]\n",
    "y_val = y[N_train:]\n",
    "N_val = len(x_val)\n",
    "print('N_train=',N_train,'  N_val=',N_val,'  L=',L,'  n_class=',n_class)\n",
    "\n",
    "# Keras wants an additional dimension with a 1 at the end\n",
    "x_train = x_train.reshape(x_train.shape[0], L, 1)\n",
    "x_val =  x_val.reshape(x_val.shape[0], L, 1)\n",
    "input_shape = (L, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...LESSON ...\n",
    "# reproducibility\n",
    "np.random.seed(12345)\n",
    "import tensorflow.random as tf_r\n",
    "tf_r.set_seed(12345)\n",
    "\n",
    "NCONV=1\n",
    "\n",
    "def build_model(NCONV=1, NF=5,KS=6, NF2=5,KS2=4, lamb=0):\n",
    "    \"\"\"\n",
    "    filter = kernel\n",
    "    NCONV=index of the CNN architecture\n",
    "    NF=nr of filters in the 1st layer\n",
    "    KS=kernel size in the 1st layer\n",
    "    NF2=nr of filters in the 2nd layer\n",
    "    KS2=kernel size in the 2nd layer\n",
    "    lamb=not the animal, but lambda, the regularization parameter\n",
    "    \"\"\"\n",
    "    # regularizers, l2=Ridge, l1=LASSO\n",
    "    reg = keras.regularizers.l2(lamb)\n",
    "    model = Sequential()\n",
    "\n",
    "    # Version with last layer Dense\n",
    "    if NCONV==1:\n",
    "        model.add(Conv1D(filters=5, \n",
    "                         kernel_size=KS,\n",
    "                         kernel_regularizer=reg,\n",
    "                         activation='relu', \n",
    "                         input_shape=input_shape))\n",
    "        model.add(MaxPooling1D(3))\n",
    "        #model.add(AveragePooling1D(3))\n",
    "        model.add(Conv1D(filters=NF2, kernel_size=KS2,activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    # Version with last layer GlobalMaxPooling, one more intermediate conv1D, and no intermediate pooling\n",
    "    if NCONV==2:\n",
    "        model.add(Conv1D(filters=NF, \n",
    "                         kernel_size=KS,\n",
    "                         kernel_regularizer=reg,\n",
    "                         activation='relu', \n",
    "                         input_shape=input_shape))\n",
    "        model.add(Conv1D(filters=NF2, kernel_size=KS2,activation='relu'))\n",
    "        model.add(Conv1D(filters=NF2, kernel_size=KS2,activation='relu'))\n",
    "        model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "        model.add(Flatten())\n",
    "        \n",
    "    # Last layer, with n_class units  \n",
    "    model.add(Dense(n_class, activation='softmax')) # softmax !\n",
    "    \n",
    "    # optimizers\n",
    "    # .SGD(lr=0.01, momentum=0.9, nesterov=True) # decay=1e-6,\n",
    "    # .RMSprop()\n",
    "    # .Nadam()\n",
    "    # Define the optimizer learning rate as a hyperparameter.\n",
    "    opt = keras.optimizers.Adam()\n",
    "    \n",
    "    # compile the model\n",
    "    # categorical_crossentropy, 3 output nodes\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,optimizer=opt,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=build_model(NCONV=NCONV,KS=11)\n",
    "\n",
    "\n",
    "print('----- Model',NCONV,'-----')\n",
    "print(model.summary())\n",
    "\n",
    "# optimizers\n",
    "# .SGD(lr=0.01, momentum=0.9, nesterov=True) # decay=1e-6,\n",
    "# .RMSprop()\n",
    "# .Nadam()\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# compile the model\n",
    "# categorical_crossentropy, 3 output nodes\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Show_weights(model,l=0,label=\"model\"):\n",
    "    c=['r','y','c','b','m','k',\"gray\",\"cyan\",\"g\"]\n",
    "    m=['o','s','D','<','>','^','+','x','d',\"*\"]\n",
    "    ms=10\n",
    "    \n",
    "    w = model.layers[l].get_weights()[0]\n",
    "    wT=w.T\n",
    "    M=len(wT)\n",
    "    b = model.layers[l].get_weights()[1]\n",
    "    \n",
    "    fig,AX=plt.subplots(1,2,figsize=(12,4.4))\n",
    "    ax=AX[0]\n",
    "    ax.axhline(0, c=\"k\")\n",
    "    ax.plot((0,))\n",
    "    for i in range(M):\n",
    "        ax.plot(wT[i][0],\"-\",c=c[i%len(c)],marker=m[i%len(m)],label=str(i),markersize=ms)\n",
    "    ax.set_title(label+': filters of layer '+str(l))\n",
    "    ax.set_xlabel('index')\n",
    "    ax=AX[1]\n",
    "    ax.axhline(0, c=\"k\")\n",
    "    for i in range(M):\n",
    "        ax.plot((i),(b[i]),c=c[i%len(c)],marker=m[i%len(m)],label=\"filter \"+str(i),markersize=ms)\n",
    "    ax.set_title(label+': bias of layer '+str(l))\n",
    "    ax.set_xlabel('filter nr')\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "Show_weights(model,0)\n",
    "Show_weights(model,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ...LESSON ...\n",
    "# Hyper-parameters\n",
    "# with small minibatch it does not converge!! \n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 300\n",
    "\n",
    "print('----- Model ',NCONV,'-----\\nFITTING....')\n",
    "fit = model.fit(x_train,y_train,batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=2, shuffle=True) \n",
    "print(\"end of fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Show_history(fit):\n",
    "    fig,AX=plt.subplots(1,2,figsize=(12,5.))\n",
    "    ax=AX[0]\n",
    "    ax.plot(fit.history['accuracy'],\"b\",label=\"train\")\n",
    "    ax.plot(fit.history['val_accuracy'],\"r--\",label=\"valid.\")\n",
    "    ax.plot((0,EPOCHS),(1/3,1/3),\":\",c=\"gray\",label=\"random choice\")\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "    ax=AX[1]\n",
    "    ax.plot(fit.history['loss'],\"b\",label=\"train\")\n",
    "    ax.plot(fit.history['val_loss'],\"r--\",label=\"valid.\")\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_ylim([0, 1.05*np.max(fit.history['loss'])])\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "Show_history(fit)\n",
    "Show_weights(model,0)\n",
    "Show_weights(model,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "LABELS = [\"Class 0\",\"Class 1\",\"Class 2\"]\n",
    "\n",
    "def show_confusion_matrix(validations, predictions, label=\"Model\",perc=False):\n",
    "\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    M = matrix\n",
    "    f = 'd'\n",
    "    cmap=\"GnBu\"\n",
    "    if perc:\n",
    "        M=M/np.sum(M)\n",
    "        f='.2%'\n",
    "        cmap='Blues'\n",
    "    seaborn.heatmap(M,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt=f,\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                cmap=cmap)\n",
    "    plt.title(label+': Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "y_pred_val = np.array(model.predict(x_val))\n",
    "# Take the class with the highest probability from the val predictions\n",
    "max_y_pred_val = np.argmax(y_pred_val, axis=1)\n",
    "max_y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_val, max_y_pred_val)\n",
    "show_confusion_matrix(max_y_val, max_y_pred_val,perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.clone_model(model)\n",
    "\n",
    "reg2 = keras.regularizers.l2(0.5)\n",
    "model2.layers[0].kernel_regularizer = reg2\n",
    "model2.layers[1].kernel_regularizer = reg2\n",
    "model2.layers[2].kernel_regularizer = reg2\n",
    "\n",
    "opt = tf.keras.optimizers.legacy.Adam()\n",
    "model2.compile(loss=keras.losses.categorical_crossentropy,optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE2 = 200\n",
    "EPOCHS2 = 300\n",
    "\n",
    "fit2 = model2.fit(x_train,y_train,batch_size=BATCH_SIZE2,epochs=EPOCHS2,\n",
    "                validation_data=(x_val, y_val), verbose=2, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Show_history(fit)\n",
    "Show_history(fit2)\n",
    "\n",
    "Show_weights(model,0,label=\"model1\")\n",
    "Show_weights(model2,0,label=\"model2\")\n",
    "Show_weights(model,2,label=\"model1\")\n",
    "Show_weights(model2,2,label=\"model2\")\n",
    "\n",
    "\n",
    "y_pred_val2 = model2.predict(x_val)\n",
    "# Take the class with the highest probability from the val predictions\n",
    "max_y_pred_val2 = np.argmax(y_pred_val2, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_val, max_y_pred_val, label=\"Model 1\")\n",
    "show_confusion_matrix(max_y_val, max_y_pred_val2, label=\"Model 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
