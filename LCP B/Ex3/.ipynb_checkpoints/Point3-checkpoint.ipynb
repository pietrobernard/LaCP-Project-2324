{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda05d5-58f4-45b3-8f1f-ad1b4fbb5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# AdaBoost Algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Gradient Boosting \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# XGBoost \n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance, to_graphviz, plot_tree\n",
    "print(\"XGBoost version:\",xgboost.__version__)\n",
    "# Other things\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, make_scorer, mean_squared_error, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "mycmap = \"winter\"\n",
    "mpl.rcParams['image.cmap'] = mycmap\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "#plt.rcParams['font.size'] = 11\n",
    "\n",
    "# customizing a little\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432fd89-124a-4ab9-8338-6dcf59511875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def plot_auc_roc_curve(y_test, y_pred, ax, name):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax, name=name)\n",
    "    #roc_display.figure_.set_size_inches(5,5)\n",
    "    ax.plot([0, 1], [0, 1], color = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e215e0-ecb6-4e87-9249-337006bd3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "dname=\"./DATA/\"\n",
    "str0=\"_XGB_24.dat\"\n",
    "fnamex=dname+'x'+str0\n",
    "fnamey=dname+'y'+str0\n",
    "x = np.loadtxt(fnamex, delimiter=\" \",dtype=float)\n",
    "y = np.loadtxt(fnamey)\n",
    "y = y.astype(int)\n",
    "N,L = len(x), len(x[0])\n",
    "\n",
    "N_train = int(0.75*N)\n",
    "x_train,y_train = x[:N_train],y[:N_train]\n",
    "x_test,y_test = x[N_train:],y[N_train:]\n",
    "print(f\"N={N}, N_train={N_train}, L={L}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf3c43b-8edf-4120-9689-7683318f7b4a",
   "metadata": {},
   "source": [
    "### Point 3 - XGBoost vs NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebadeac-5486-4588-83a1-dd50faeb49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the data\n",
    "np.random.seed(12345)\n",
    "\n",
    "dname=\"./DATA/\"\n",
    "str0=\"_XGB_24.dat\"\n",
    "fnamex=dname+'x'+str0\n",
    "fnamey=dname+'y'+str0\n",
    "x = np.loadtxt(fnamex, delimiter=\" \",dtype=float)\n",
    "y = np.loadtxt(fnamey)\n",
    "y = y.astype(int)\n",
    "N,L = len(x), len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f304e0c-be89-47b9-a9e4-5438055ab181",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ae46e-5db8-462d-84b6-81005ababe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=10**5)\n",
    "stats_NN = []\n",
    "for train_fraction in np.linspace(0.1, 0.75, 25):\n",
    "    # splitting the dataset\n",
    "    N_train = int(train_fraction*N)\n",
    "    x_train,y_train = x[:N_train],y[:N_train]\n",
    "    x_test,y_test = x[N_train:],y[N_train:]\n",
    "    # scaling the data since NN are sensible to this\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)  \n",
    "    x_train = scaler.transform(x_train)  \n",
    "    x_test = scaler.transform(x_test)\n",
    "    # cross-validating with 5 folds\n",
    "    kf = KFold(n_splits=5)\n",
    "    fold_accs = {'train':[], 'test':[], 'valid':[]}\n",
    "    fold_results = []\n",
    "    print(\"Train fraction:\",train_fraction)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        # fitting the model\n",
    "        model_NN.fit(x_train[train_index], y_train[train_index])\n",
    "        # saving the accuracies\n",
    "        fold_accs['train'].append( accuracy_score(y_train[train_index], model_NN.predict(x_train[train_index])) )\n",
    "        fold_accs['test'].append( accuracy_score(y_train[test_index], model_NN.predict(x_train[test_index])) ) \n",
    "        fold_accs['valid'].append( accuracy_score(y_test, model_NN.predict(x_test)) )\n",
    "    \n",
    "    # computing mean training and test accuracies on the various folds\n",
    "    fold_accs['train'] = np.array(fold_accs['train'])\n",
    "    fold_accs['test'] = np.array(fold_accs['test'])\n",
    "    stats_NN.append( [np.mean(fold_accs['train']), np.std(fold_accs['train']), np.mean(fold_accs['test']), np.std(fold_accs['test']), np.mean(fold_accs['valid']), np.std(fold_accs['valid'])] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889fb4e-d542-459b-afcd-940d3895c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_NN = np.array(stats_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbceeb-7745-4993-8631-468aff5a17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold\n",
    "parameters = {'colsample_bytree': 1,\n",
    " 'device': 'cuda',\n",
    " 'eta': 0.3,\n",
    " 'eval_metric': ['logloss', 'error'],\n",
    " 'gamma': 1.6071428571428572,\n",
    " 'importance_type': 'gain',\n",
    " 'lambda': 0.5555555555555556,\n",
    " 'max_depth': 9,\n",
    " 'min_child_weight': 1,\n",
    " 'n_estimators': 10,\n",
    " 'objective': 'binary:logistic',\n",
    " 'seed': 1,\n",
    " 'subsample': 1,\n",
    " 'verbosity': 0}\n",
    "model = XGBClassifier(**parameters)\n",
    "stats_XGB = []\n",
    "\n",
    "for train_fraction in np.linspace(0.1, 0.75, 25):\n",
    "    # splitting the dataset\n",
    "    N_train = int(train_fraction*N)\n",
    "    x_train,y_train = x[:N_train],y[:N_train]\n",
    "    x_test,y_test = x[N_train:],y[N_train:]\n",
    "    print(f\"N={N}, N_train={N_train}, L={L}\")\n",
    "    # doing stuff\n",
    "    fold_accs = {'train':[],'test':[],'valid':[]}\n",
    "    kf = KFold(n_splits=5)\n",
    "    print(\"Train fraction:\",train_fraction)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        # fitting the model\n",
    "        ev_set = [(x_train[train_index], y_train[train_index]), (x_train[test_index], y_train[test_index])]\n",
    "        model.fit(x_train[train_index], y_train[train_index], verbose=False, eval_set=ev_set)\n",
    "        # saving the accuracies\n",
    "        fold_accs['train'].append( accuracy_score(y_train[train_index], model.predict(x_train[train_index])) )\n",
    "        fold_accs['test'].append( accuracy_score(y_train[test_index], model.predict(x_train[test_index])) ) \n",
    "        fold_accs['valid'].append( accuracy_score( y_test, model.predict(x_test) ) )\n",
    "    \n",
    "    # computing mean training and test accuracies on the various folds\n",
    "    fold_accs['train'] = np.array(fold_accs['train'])\n",
    "    fold_accs['test'] = np.array(fold_accs['test'])\n",
    "    fold_accs['valid'] = np.array(fold_accs['valid'])\n",
    "    stats_XGB.append( [np.mean(fold_accs['train']), np.std(fold_accs['train']), np.mean(fold_accs['test']), np.std(fold_accs['test']), np.mean(fold_accs['valid']), np.std(fold_accs['valid'])] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffb2b5-c0a9-4006-8045-8deb4be8f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_XGB = np.array(stats_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba2b3f-986b-4c3e-991b-e20f0d9f748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(8,4), nrows=1, ncols=2)\n",
    "axs[0].plot(np.linspace(0.1, 0.75, 25), stats_XGB[:,0], color='red', linestyle='dotted', linewidth=0.5)\n",
    "axs[0].errorbar(np.linspace(0.1, 0.75, 25), stats_XGB[:,0], yerr=stats_XGB[:,1], fmt='o', markersize=3, capsize=2, color='red', linewidth=1, label='Train Accuracy')\n",
    "axs[0].plot(np.linspace(0.1, 0.75, 25), stats_XGB[:,2], color='black', linestyle='dotted', linewidth=0.5)\n",
    "axs[0].errorbar(np.linspace(0.1, 0.75, 25), stats_XGB[:,2], yerr=stats_XGB[:,3], fmt='s', markersize=3, capsize=2, color='black', linewidth=1, label='Test Accuracy')\n",
    "axs[0].set_xlabel('Training Set Fraction of Whole Dataset')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlim(0.08,0.76)\n",
    "axs[0].set_xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n",
    "axs[0].set_title(\"XGBoost Model - Optimal Parameters\")\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "axs[1].plot(np.linspace(0.1, 0.75, 25), stats_NN[:,0], color='red', linestyle='dotted', linewidth=0.5)\n",
    "axs[1].errorbar(np.linspace(0.1, 0.75, 25), stats_NN[:,0], yerr=stats_NN[:,1], fmt='o', markersize=3, capsize=2, color='red', linewidth=1, label='Train Accuracy')\n",
    "axs[1].plot(np.linspace(0.1, 0.75, 25), stats_NN[:,2], color='black', linestyle='dotted', linewidth=0.5)\n",
    "axs[1].errorbar(np.linspace(0.1, 0.75, 25), stats_NN[:,2], yerr=stats_NN[:,3], fmt='s', markersize=3, capsize=2, color='black', linewidth=1, label='Test Accuracy')\n",
    "axs[1].set_xlabel('Training Set Fraction of Whole Dataset')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].set_xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n",
    "axs[1].set_title(\"MLP Classifier\")\n",
    "axs[1].legend()\n",
    "\n",
    "fig.suptitle(\"XGBoost vs MLP Classifier\\nTrain and Test accuracies - Cross Validation (5 folds)\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788276e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
