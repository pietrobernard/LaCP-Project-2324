{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36e3d44",
   "metadata": {},
   "source": [
    "## LCPB 23-24 exercise 4 (Restricted Boltzmann Machine)\n",
    "\n",
    "- Andrea Semenzato 2130973\n",
    "- Pietro Bernardi 2097494\n",
    "- Tomàs Mezquita 2109239\n",
    "- Mariam Chokheli 2122278\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1191a4c",
   "metadata": {},
   "source": [
    "We want to study the performances of an RBM, and, by looking at its learned weights and biases,\n",
    "better understand the correlations in the data (from file x_RBM_2024_exercise.dat, $N=10^4$\n",
    "configurations with L=10 bits). Use an RBM with M=3 hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8dc58",
   "metadata": {},
   "source": [
    "### Point 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5dc3a6",
   "metadata": {},
   "source": [
    "Increase the number of contrastive divergence steps from n=1 to n=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4ce1b",
   "metadata": {},
   "source": [
    "Referring to the code that was provided in the notebook, we added this loop inside the k-loop that repeats the forward and backward passes between the visible and hidden layers $CDK$-times, where $CDK = 5$ at this time as requested.\n",
    "\n",
    "```python\n",
    "h = activate(v[k], w, b, GAP)\n",
    "hf = h\n",
    "for cdk_it in range(CDK):\n",
    "    vf = activate(hf, w.T, a, GAP) #changed to hf from h\n",
    "    hf = activate(vf, w, b, GAP)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef676a",
   "metadata": {},
   "source": [
    "### Point 2\n",
    "Compute the log-likelihood $\\mathcal{L}$ during the training, at every epoch, or every minibatch update if it\n",
    "reaches a maximum already in the first epoch. Use “t” as an index of this “time”, indicating the unit\n",
    "in the figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420ba45",
   "metadata": {},
   "source": [
    "We computed the log-likelihood by number of epochs and by minibatch. The results are shown below in the case of $M=3$ and $CD=5$.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./ex_img/loglike_cd5_m3.png\" width=800>\n",
    "    Figure 1\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48063259",
   "metadata": {},
   "source": [
    "The log-likelihood was computed via these functions:\n",
    "\n",
    "##### getEnergy\n",
    "This function computes the energy for a given configuration (h,v) as per:\n",
    "$$\n",
    "    E(v,h) = -v\\cdot a - h\\cdot b - v^T\\cdot W\\cdot h\n",
    "$$\n",
    "```python\n",
    "def getEnergy(h, v, w, a, b):\n",
    "# a : bias when back passing : it is the visible bias\n",
    "# b : bias when forward passing : it is the hidden bias\n",
    "e0 = np.dot(np.matmul(v.T, w), h)\n",
    "e1 = np.dot(a.T, v)\n",
    "e2 = np.dot(b.T, h)\n",
    "return -1.*(e0+e1+e2)\n",
    "```\n",
    "\n",
    "##### getMeanEnergyData\n",
    "This function computes $<E>_{data}$ via:\n",
    "$$\n",
    "    <E>_{data} = \\frac{1}{N}\\sum_{n} <E(v_n,h)> = \\frac{1}{N}\\sum_{n} \\left(\\frac{\\sum_h E(v_n,h)\\cdot e^{-E(v_n,h)}}{\\sum_h e^{-E(v_n,h)}}\\right)\n",
    "$$\n",
    "where $N$ is the number of vectors being considered ($N=10^4$ in the case of the energy-per-epoch estimate, but it is equal to the batch size in case of the batch estimate).\n",
    "\n",
    "```python\n",
    "# w, a, b : model parameters\n",
    "# v : input vectors\n",
    "# k_start, k_end : range of input vectors to consider\n",
    "def getMeanEnergyData(w, a, b, v, k_start=0, k_end=N, M=3):\n",
    "    # getting all the h configurations\n",
    "    hs = list(it.product((0,1), repeat=M))\n",
    "    e = 0\n",
    "    # checking to not overshoot the number of input vectors\n",
    "    k_end_limit = k_end\n",
    "    if k_end >= N:\n",
    "        k_end_limit = N\n",
    "    for k in range(k_start,k_end_limit,1):\n",
    "        # foreach input vector v[k]\n",
    "        e_num = 0\n",
    "        e_den = 0\n",
    "        for h in hs:\n",
    "            en = getEnergy(h, v[k], w, a, b)\n",
    "            boltz = np.exp(-1.0*en)\n",
    "            e_num += (en*boltz)\n",
    "            e_den += boltz\n",
    "        # now we have the <E(v_n,h)>\n",
    "        e += (e_num/e_den)\n",
    "    return e/(k_end_limit-k_start)\n",
    "```\n",
    "\n",
    "##### getPartitionFunction\n",
    "This function computes the partition function by considering all of the $L+M$ configurations.\n",
    "```python\n",
    "def getPartitionFunction(w, a, b, L=10, M=3):\n",
    "    # generating the configurations\n",
    "    confs = list(it.product((0,1), repeat=(L+M)))\n",
    "    e_cfg = 0\n",
    "    for cfg in confs:\n",
    "        v = np.array(cfg[:L])\n",
    "        h = np.array(cfg[L:])\n",
    "        e_cfg += np.exp(-1.0*getEnergy(h, v, w, a, b))\n",
    "    # now we have the sum of all terms, hence:\n",
    "    return e_cfg\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210390c",
   "metadata": {},
   "source": [
    "The following figure shows how the model's parameters are adjusted during the training procedure for a model with $M=3$, over $100$ epochs:\n",
    "<div align=\"center\">\n",
    "    <img src=\"./ex_img/anim_m3_cd5.gif\" width=400>\n",
    "    Figure 2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3e73d",
   "metadata": {},
   "source": [
    "### Point 3\n",
    "Try RBMs with different numbers of hidden units: M=1, 2, 3 (done above), 4, 5, and 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1b6f7",
   "metadata": {},
   "source": [
    "The log likelihoods with $M=1,2,4,5,6$ are reported below:\n",
    "\n",
    "<div align=\"center\">\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./ex_img/loglike_cd5_m1.png\" width=600></td>\n",
    "        <td><img src=\"./ex_img/m1_cd5/img_100.png\" width=400>\n",
    "        <br>Weights after $100$ epochs for $CD=5, M=1$.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"./ex_img/loglike_cd5_m2.png\" width=600></td>\n",
    "        <td><img src=\"./ex_img/m2_cd5/img_100.png\" width=400>\n",
    "        <br>Weights after $100$ epochs for $CD=5, M=2$.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"./ex_img/loglike_cd5_m3.png\" width=600></td>\n",
    "        <td><img src=\"./ex_img/m3_cd5/img_100.png\" width=400>\n",
    "        <br>Weights after $100$ epochs for $CD=5, M=3$.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"./ex_img/loglike_cd5_m4.png\" width=600></td>\n",
    "        <td><img src=\"./ex_img/m4_cd5/img_100.png\" width=400>\n",
    "        <br>Weights after $100$ epochs for $CD=5, M=4$.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"./ex_img/loglike_cd5_m5.png\" width=600></td>\n",
    "        <td><img src=\"./ex_img/m5_cd5/img_100.png\" width=400>\n",
    "        <br>Weights after $100$ epochs for $CD=5, M=5$.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"./ex_img/loglike_cd5_m6.png\" width=600></td>\n",
    "        <td><img src=\"./ex_img/m6_cd5/img_100.png\" width=400>\n",
    "        <br>Weights after $100$ epochs for $CD=5, M=6$.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "    Figure 3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4ecfb",
   "metadata": {},
   "source": [
    "We observed that with increasing $M$ the log-likelihood tends to stabilize at lower values and also for $M > 4$ it exhibits serious fluctuations. Also, the plots suggest that for $M\\geq 3$, a greater number of epochs might be required for the log-likelihood to stabilize. We also notice that for $M\\leq 2$, not all the input nodes are strongly linked to the hidden layer nodes. This might indicate that the inner layer is too simple to fully capture the patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b9a84",
   "metadata": {},
   "source": [
    "### Point 4\n",
    "For $M=3$, plot $\\mathcal{L}$ as a function of “t”, comparing the two contrastive divergence cases (n=1 and\n",
    "n=5). Then, for n=1, plot $\\mathcal{L}$ as a function of “t”, comparing the two cases with different M."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7fea3",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"./ex_img/ll_m3_cd15.png\" width=600>\n",
    "    Figure 4\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd89be",
   "metadata": {},
   "source": [
    "The result of the comparison is shown in figure 4. The log likelihoods for the two cases of contrastive divergence look almost the same, both when calculated for each epoch and each minibatch. We then compared, with $CD=1$, the case with $M=3$ with the other values of $M$.\n",
    "\n",
    "In the plot below, the case with $M=3$ is always drawn in blue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554554e1",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td><img src=\"./ex_img/ll_m1_m3_cd1.png\" width=600></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><img src=\"./ex_img/ll_m2_m3_cd1.png\" width=600></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><img src=\"./ex_img/ll_m4_m3_cd1.png\" width=600></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><img src=\"./ex_img/ll_m5_m3_cd1.png\" width=600></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><img src=\"./ex_img/ll_m6_m3_cd1.png\" width=600></td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    Figure 5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29536f85",
   "metadata": {},
   "source": [
    "### Point 5\n",
    "From the weights learned by the RBM, guess the structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5dedcc",
   "metadata": {},
   "source": [
    "We decided to augment the number of epochs to $200$ when training the $M=3$ case. In order to decide which hidden layer configuration was best, we computed the distribution of the errors for each input vector with respect to the vectors generated by the RBM. We expected that such distribution be centered around zero if the RBM was properly structured and trained. We found that the distribution is systematically shifted towards $1/2$.\n",
    "There was little difference with respect to this behaviour between $M=3$ and $M=4$ cases and thus we have chosen $M=3$ since the model is simpler and thus faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89ef8b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./ex_img/rbm_m3_errors.png\" width=400>\n",
    "            </td>\n",
    "            <td>\n",
    "                <img src=\"./ex_img/rbm_m3_200.png\" width=400>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    Figure 6: the red color is used to signal positive bias/weight while the blu signals negative.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d32cb",
   "metadata": {},
   "source": [
    "Figure 6 shows the errors on the left and the RBM structure on the right. We see from the plot that the structure of the data is as follows (counting from left to right with index ranging from 0 upwards):\n",
    "\n",
    "| Hidden layer node | Strongly linked to input nodes|\n",
    "| :- | -: |\n",
    "| 0 | 5, 6, 8, 9 |\n",
    "| 1 | 0, 3 |\n",
    "| 2 | 1, 2, 4, 7 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
